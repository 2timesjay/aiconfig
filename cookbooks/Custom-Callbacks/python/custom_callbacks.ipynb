{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacobjensen/.pyenv/versions/aiconfig/lib/python3.10/site-packages/pydantic/_internal/_fields.py:149: UserWarning: Field \"model_parsers\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/jacobjensen/.pyenv/versions/aiconfig/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import openai\n",
    "import os\n",
    "from aiconfig.Config import AIConfigRuntime\n",
    "\n",
    "# #Set the OpenAI key. The following code is specific for Google Colab\n",
    "# #We recommend using environment variables for setting the open ai key\n",
    "# from google.colab import userdata\n",
    "# openai.api_key = userdata.get('OPENAI_API_KEY')\n",
    "openai.api_key = open(\"/home/jacobjensen/secrets/openai_api_key.txt\", \"r\").read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiconfig import CallbackEvent\n",
    "import pprint\n",
    "async def my_custom_callback(event: CallbackEvent) -> None:\n",
    "  \"\"\"\n",
    "    This is a custom callback that prints the event name and the event stdout.\n",
    "\n",
    "    Args:\n",
    "        event (CallbackEvent): The event that triggered the callback.\n",
    "\n",
    "  \"\"\"\n",
    "  print(f\"Event triggered: {event.name}\")\n",
    "  pprint.pprint(event, width = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiconfig import AIConfigRuntime\n",
    "\n",
    "config = AIConfigRuntime.load(\"../travel.aiconfig.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ChatCompletionChunk' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jacobjensen/Projects/llms/aiconfig/cookbooks/Custom-Callbacks/python/custom_callbacks.ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jacobjensen/Projects/llms/aiconfig/cookbooks/Custom-Callbacks/python/custom_callbacks.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maiconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m CallbackManager\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jacobjensen/Projects/llms/aiconfig/cookbooks/Custom-Callbacks/python/custom_callbacks.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# callback_manager = CallbackManager([my_custom_callback])\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jacobjensen/Projects/llms/aiconfig/cookbooks/Custom-Callbacks/python/custom_callbacks.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# config.set_callback_manager(callback_manager)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jacobjensen/Projects/llms/aiconfig/cookbooks/Custom-Callbacks/python/custom_callbacks.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mawait\u001b[39;00m config\u001b[39m.\u001b[39mrun(prompt_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mget_activities\u001b[39m\u001b[39m\"\u001b[39m, run_with_dependencies\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Projects/llms/aiconfig/python/src/aiconfig/Config.py:245\u001b[0m, in \u001b[0;36mAIConfigRuntime.run\u001b[0;34m(self, prompt_name, params, options, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_model_name(prompt_data)\n\u001b[1;32m    243\u001b[0m model_provider \u001b[39m=\u001b[39m AIConfigRuntime\u001b[39m.\u001b[39mget_model_parser(model_name)\n\u001b[0;32m--> 245\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m model_provider\u001b[39m.\u001b[39mrun(prompt_data, \u001b[39mself\u001b[39m, options, params, callback_manager \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    247\u001b[0m event \u001b[39m=\u001b[39m CallbackEvent(\u001b[39m\"\u001b[39m\u001b[39mon_run_complete\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m__name__\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m: response})\n\u001b[1;32m    248\u001b[0m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mrun_callbacks(event)\n",
      "File \u001b[0;32m~/Projects/llms/aiconfig/python/src/aiconfig/default_parsers/parameterized_model_parser.py:59\u001b[0m, in \u001b[0;36mParameterizedModelParser.run\u001b[0;34m(self, prompt, aiconfig, options, parameters, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\n\u001b[1;32m     50\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     51\u001b[0m     prompt: Prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ExecuteResult:\n\u001b[1;32m     57\u001b[0m     \u001b[39m# maybe use prompt metadata instead of kwargs?\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_with_dependencies\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> 59\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_with_dependencies(prompt, aiconfig, options, parameters)\n\u001b[1;32m     60\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_inference(prompt, aiconfig, options, parameters)\n",
      "File \u001b[0;32m~/Projects/llms/aiconfig/python/src/aiconfig/default_parsers/parameterized_model_parser.py:107\u001b[0m, in \u001b[0;36mParameterizedModelParser.run_with_dependencies\u001b[0;34m(self, prompt, aiconfig, options, parameters)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[39mif\u001b[39;00m prompt_name \u001b[39m==\u001b[39m prompt\u001b[39m.\u001b[39mname:\n\u001b[1;32m    105\u001b[0m         \u001b[39mreturn\u001b[39;00m output\n\u001b[0;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m execute_recursive(prompt\u001b[39m.\u001b[39mname)\n",
      "File \u001b[0;32m~/Projects/llms/aiconfig/python/src/aiconfig/default_parsers/parameterized_model_parser.py:101\u001b[0m, in \u001b[0;36mParameterizedModelParser.run_with_dependencies.<locals>.execute_recursive\u001b[0;34m(prompt_name)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mfor\u001b[39;00m dependency_prompt_name \u001b[39min\u001b[39;00m dependency_graph[prompt_name]:\n\u001b[1;32m     99\u001b[0m     \u001b[39mawait\u001b[39;00m execute_recursive(dependency_prompt_name)\n\u001b[0;32m--> 101\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m aiconfig\u001b[39m.\u001b[39mrun(prompt_name, parameters, options)\n\u001b[1;32m    103\u001b[0m \u001b[39m# Return the output of the original prompt being executed\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m prompt_name \u001b[39m==\u001b[39m prompt\u001b[39m.\u001b[39mname:\n",
      "File \u001b[0;32m~/Projects/llms/aiconfig/python/src/aiconfig/Config.py:245\u001b[0m, in \u001b[0;36mAIConfigRuntime.run\u001b[0;34m(self, prompt_name, params, options, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_model_name(prompt_data)\n\u001b[1;32m    243\u001b[0m model_provider \u001b[39m=\u001b[39m AIConfigRuntime\u001b[39m.\u001b[39mget_model_parser(model_name)\n\u001b[0;32m--> 245\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m model_provider\u001b[39m.\u001b[39mrun(prompt_data, \u001b[39mself\u001b[39m, options, params, callback_manager \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    247\u001b[0m event \u001b[39m=\u001b[39m CallbackEvent(\u001b[39m\"\u001b[39m\u001b[39mon_run_complete\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m__name__\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m: response})\n\u001b[1;32m    248\u001b[0m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mrun_callbacks(event)\n",
      "File \u001b[0;32m~/Projects/llms/aiconfig/python/src/aiconfig/default_parsers/parameterized_model_parser.py:61\u001b[0m, in \u001b[0;36mParameterizedModelParser.run\u001b[0;34m(self, prompt, aiconfig, options, parameters, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_with_dependencies(prompt, aiconfig, options, parameters)\n\u001b[1;32m     60\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_inference(prompt, aiconfig, options, parameters)\n",
      "File \u001b[0;32m~/Projects/llms/aiconfig/python/src/aiconfig/default_parsers/openai.py:277\u001b[0m, in \u001b[0;36mOpenAIInference.run_inference\u001b[0;34m(self, prompt, aiconfig, options, parameters)\u001b[0m\n\u001b[1;32m    274\u001b[0m response \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mmodel_dump(exclude_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \\\n\u001b[1;32m    275\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(response, BaseModel) \u001b[39melse\u001b[39;00m response\n\u001b[1;32m    276\u001b[0m \u001b[39m# streaming only returns one chunk, one choice at a time (before 1.0.0). The order in which the choices are returned is not guaranteed.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m messages \u001b[39m=\u001b[39m multi_choice_message_reducer(messages, chunk)\n\u001b[1;32m    279\u001b[0m \u001b[39mfor\u001b[39;00m i, choice \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(chunk[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[1;32m    280\u001b[0m     index \u001b[39m=\u001b[39m choice\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/llms/aiconfig/python/src/aiconfig/default_parsers/openai.py:392\u001b[0m, in \u001b[0;36mmulti_choice_message_reducer\u001b[0;34m(messages, chunk)\u001b[0m\n\u001b[1;32m    387\u001b[0m     messages \u001b[39m=\u001b[39m {}\n\u001b[1;32m    389\u001b[0m \u001b[39m# elif len(messages) != len(chunk[\"choices\"]):\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[39m#     raise ValueError(\"Invalid number of previous choices -- it should match the incoming number of choices\")\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m \u001b[39mfor\u001b[39;00m choice \u001b[39min\u001b[39;00m chunk[\u001b[39m\"\u001b[39;49m\u001b[39mchoices\u001b[39;49m\u001b[39m\"\u001b[39;49m]:\n\u001b[1;32m    393\u001b[0m     index \u001b[39m=\u001b[39m choice[\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    394\u001b[0m     previous_message \u001b[39m=\u001b[39m messages\u001b[39m.\u001b[39mget(index, {})\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ChatCompletionChunk' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from aiconfig import CallbackManager\n",
    "\n",
    "# callback_manager = CallbackManager([my_custom_callback])\n",
    "# config.set_callback_manager(callback_manager)\n",
    "\n",
    "await config.run(prompt_name=\"get_activities\", run_with_dependencies=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
